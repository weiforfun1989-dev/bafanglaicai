#!/usr/bin/env python3
"""
===================================
Nitter Twitter Search - Trump æ¨æ–‡æœç´¢
===================================

åŠŸèƒ½ï¼š
1. ä½¿ç”¨ Nitter æœç´¢ Trump çš„æ¨æ–‡
2. æ— éœ€ Twitter API Key
3. è¿”å›æ¨æ–‡å†…å®¹ã€æ—¶é—´ã€ç‚¹èµ/è½¬å‘æ•°

ä½¿ç”¨æ–¹å¼ï¼š
    python nitter_search.py                    # æœç´¢ Trump æœ€æ–°æ¨æ–‡
    python nitter_search.py --query "tariff"   # æœç´¢ç‰¹å®šå…³é”®è¯
    python nitter_search.py --limit 20         # è·å–20æ¡æ¨æ–‡
"""

import argparse
import logging
import re
import time
from dataclasses import dataclass
from datetime import datetime
from typing import List, Optional
from urllib.parse import quote

import requests
from bs4 import BeautifulSoup

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Nitter å®ä¾‹åˆ—è¡¨ï¼ˆæŒ‰å¯é æ€§æ’åºï¼‰
NITTER_INSTANCES = [
    "https://nitter.net",
    "https://nitter.it",
    "https://nitter.cz",
    "https://nitter.privacydev.net",
    "https://nitter.projectsegfault.com",
]


@dataclass
class Tweet:
    """æ¨æ–‡æ•°æ®ç±»"""
    id: str
    content: str
    created_at: str
    likes: int
    retweets: int
    replies: int
    url: str
    media_urls: List[str]
    is_reply: bool = False
    is_retweet: bool = False


class NitterClient:
    """Nitter å®¢æˆ·ç«¯"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.base_url = None
        self._find_working_instance()
    
    def _find_working_instance(self):
        """æ‰¾åˆ°å¯ç”¨çš„ Nitter å®ä¾‹"""
        for instance in NITTER_INSTANCES:
            try:
                logger.info(f"æµ‹è¯• Nitter å®ä¾‹: {instance}")
                response = self.session.get(instance, timeout=10)
                if response.status_code == 200:
                    self.base_url = instance
                    logger.info(f"âœ… ä½¿ç”¨ Nitter å®ä¾‹: {instance}")
                    return
            except Exception as e:
                logger.warning(f"âŒ {instance} ä¸å¯ç”¨: {e}")
                continue
        
        raise Exception("æ²¡æœ‰å¯ç”¨çš„ Nitter å®ä¾‹")
    
    def get_user_tweets(self, username: str = "realDonaldTrump", limit: int = 20) -> List[Tweet]:
        """
        è·å–ç”¨æˆ·çš„æ¨æ–‡
        
        Args:
            username: Twitter ç”¨æˆ·å
            limit: è·å–æ•°é‡
            
        Returns:
            æ¨æ–‡åˆ—è¡¨
        """
        tweets = []
        cursor = ""
        
        while len(tweets) < limit:
            try:
                url = f"{self.base_url}/{username}"
                if cursor:
                    url += f"?cursor={cursor}"
                
                logger.info(f"è·å– {username} çš„æ¨æ–‡...")
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                
                # è§£æ HTML
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾æ¨æ–‡
                tweet_elements = soup.find_all('div', class_='timeline-item')
                
                if not tweet_elements:
                    logger.warning("æ²¡æœ‰æ‰¾åˆ°æ¨æ–‡")
                    break
                
                for element in tweet_elements:
                    try:
                        tweet = self._parse_tweet_element(element, username)
                        if tweet:
                            tweets.append(tweet)
                            
                            if len(tweets) >= limit:
                                break
                    except Exception as e:
                        logger.warning(f"è§£ææ¨æ–‡å¤±è´¥: {e}")
                        continue
                
                # æŸ¥æ‰¾ä¸‹ä¸€é¡µ cursor
                show_more = soup.find('div', class_='show-more')
                if show_more and show_more.find('a'):
                    href = show_more.find('a')['href']
                    match = re.search(r'cursor=([^&]+)', href)
                    if match:
                        cursor = match.group(1)
                    else:
                        break
                else:
                    break
                
                # é˜²å°ç¦å»¶è¿Ÿ
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"è·å–æ¨æ–‡å¤±è´¥: {e}")
                break
        
        return tweets[:limit]
    
    def search_tweets(self, query: str, limit: int = 20) -> List[Tweet]:
        """
        æœç´¢æ¨æ–‡
        
        Args:
            query: æœç´¢å…³é”®è¯
            limit: è·å–æ•°é‡
            
        Returns:
            æ¨æ–‡åˆ—è¡¨
        """
        tweets = []
        cursor = ""
        
        # URL ç¼–ç æŸ¥è¯¢è¯
        encoded_query = quote(query)
        
        while len(tweets) < limit:
            try:
                url = f"{self.base_url}/search?f=tweets&q={encoded_query}"
                if cursor:
                    url += f"&cursor={cursor}"
                
                logger.info(f"æœç´¢: {query}")
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                
                # è§£æ HTML
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾æ¨æ–‡
                tweet_elements = soup.find_all('div', class_='timeline-item')
                
                if not tweet_elements:
                    logger.warning("æ²¡æœ‰æ‰¾åˆ°æ¨æ–‡")
                    break
                
                for element in tweet_elements:
                    try:
                        tweet = self._parse_tweet_element(element)
                        if tweet:
                            tweets.append(tweet)
                            
                            if len(tweets) >= limit:
                                break
                    except Exception as e:
                        logger.warning(f"è§£ææ¨æ–‡å¤±è´¥: {e}")
                        continue
                
                # æŸ¥æ‰¾ä¸‹ä¸€é¡µ cursor
                show_more = soup.find('div', class_='show-more')
                if show_more and show_more.find('a'):
                    href = show_more.find('a')['href']
                    match = re.search(r'cursor=([^&]+)', href)
                    if match:
                        cursor = match.group(1)
                    else:
                        break
                else:
                    break
                
                # é˜²å°ç¦å»¶è¿Ÿ
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"æœç´¢æ¨æ–‡å¤±è´¥: {e}")
                break
        
        return tweets[:limit]
    
    def _parse_tweet_element(self, element, default_username: str = "") -> Optional[Tweet]:
        """
        è§£ææ¨æ–‡å…ƒç´ 
        
        Args:
            element: BeautifulSoup å…ƒç´ 
            default_username: é»˜è®¤ç”¨æˆ·å
            
        Returns:
            Tweet å¯¹è±¡æˆ– None
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¹¿å‘Šæˆ–å…¶ä»–å†…å®¹
            if element.find('div', class_='ad'):
                return None
            
            # è·å–æ¨æ–‡é“¾æ¥å’ŒID
            link_element = element.find('a', class_='tweet-link')
            if not link_element:
                return None
            
            tweet_url = link_element['href']
            if not tweet_url.startswith('http'):
                tweet_url = self.base_url + tweet_url
            
            # æå–æ¨æ–‡ID
            tweet_id = ""
            match = re.search(r'/status/(\d+)', tweet_url)
            if match:
                tweet_id = match.group(1)
            
            # è·å–ç”¨æˆ·å
            username_element = element.find('a', class_='username')
            username = username_element.text.strip() if username_element else default_username
            
            # è·å–å†…å®¹
            content_element = element.find('div', class_='tweet-content')
            if not content_element:
                return None
            
            # æå–æ–‡æœ¬
            text_element = content_element.find('div', class_='tweet-text')
            content = ""
            if text_element:
                # æ¸…ç† HTML æ ‡ç­¾
                for br in text_element.find_all('br'):
                    br.replace_with('\n')
                content = text_element.get_text(separator=' ', strip=True)
            
            # è·å–æ—¶é—´
            time_element = element.find('span', class_='tweet-date')
            created_at = ""
            if time_element and time_element.find('a'):
                # ä» title å±æ€§è·å–å®Œæ•´æ—¶é—´
                time_link = time_element.find('a')
                created_at = time_link.get('title', time_link.text.strip())
            
            # è·å–ç»Ÿè®¡æ•°æ®
            stats = element.find('div', class_='tweet-stats')
            likes = 0
            retweets = 0
            replies = 0
            
            if stats:
                # å›å¤æ•°
                reply_stat = stats.find('div', class_='icon-reply')
                if reply_stat:
                    reply_text = reply_stat.get_text(strip=True)
                    replies = self._parse_number(reply_text)
                
                # è½¬å‘æ•°
                retweet_stat = stats.find('div', class_='icon-retweet')
                if retweet_stat:
                    retweet_text = retweet_stat.get_text(strip=True)
                    retweets = self._parse_number(retweet_text)
                
                # ç‚¹èµæ•°
                like_stat = stats.find('div', class_='icon-heart')
                if like_stat:
                    like_text = like_stat.get_text(strip=True)
                    likes = self._parse_number(like_text)
            
            # è·å–åª’ä½“
            media_urls = []
            attachments = element.find('div', class_='attachments')
            if attachments:
                for img in attachments.find_all('img'):
                    if img.get('src'):
                        media_urls.append(img['src'])
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å›å¤
            is_reply = bool(element.find('div', class_='replying-to'))
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯è½¬å‘
            is_retweet = bool(element.find('div', class_='retweet-header'))
            
            return Tweet(
                id=tweet_id,
                content=content,
                created_at=created_at,
                likes=likes,
                retweets=retweets,
                replies=replies,
                url=tweet_url,
                media_urls=media_urls,
                is_reply=is_reply,
                is_retweet=is_retweet
            )
            
        except Exception as e:
            logger.warning(f"è§£ææ¨æ–‡å…ƒç´ å¤±è´¥: {e}")
            return None
    
    def _parse_number(self, text: str) -> int:
        """è§£ææ•°å­—ï¼ˆæ”¯æŒ K/M åç¼€ï¼‰"""
        text = text.strip().replace(',', '')
        
        if not text:
            return 0
        
        try:
            if text.endswith('K'):
                return int(float(text[:-1]) * 1000)
            elif text.endswith('M'):
                return int(float(text[:-1]) * 1000000)
            else:
                return int(text)
        except:
            return 0


def main():
    """ä¸»å…¥å£å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Nitter Twitter æœç´¢å·¥å…·'
    )
    
    parser.add_argument(
        '--username', '-u',
        type=str,
        default='realDonaldTrump',
        help='Twitter ç”¨æˆ·åï¼Œé»˜è®¤ realDonaldTrump'
    )
    
    parser.add_argument(
        '--query', '-q',
        type=str,
        help='æœç´¢å…³é”®è¯'
    )
    
    parser.add_argument(
        '--limit', '-l',
        type=int,
        default=10,
        help='è·å–æ•°é‡ï¼Œé»˜è®¤10æ¡'
    )
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        client = NitterClient()
        
        # è·å–æ¨æ–‡
        if args.query:
            # æœç´¢æ¨¡å¼
            search_query = f"from:{args.username} {args.query}"
            tweets = client.search_tweets(search_query, args.limit)
        else:
            # ç”¨æˆ·æ¨æ–‡æ¨¡å¼
            tweets = client.get_user_tweets(args.username, args.limit)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nğŸ“Š æ‰¾åˆ° {len(tweets)} æ¡æ¨æ–‡\n")
        print("=" * 80)
        
        for i, tweet in enumerate(tweets, 1):
            print(f"\n{i}. ", end="")
            if tweet.is_retweet:
                print("[è½¬å‘] ", end="")
            if tweet.is_reply:
                print("[å›å¤] ", end="")
            print(f"{tweet.created_at}")
            
            print(f"   ğŸ“ {tweet.content[:150]}{'...' if len(tweet.content) > 150 else ''}")
            print(f"   â¤ï¸ {tweet.likes}  ğŸ’¬ {tweet.replies}  ğŸ”„ {tweet.retweets}")
            print(f"   ğŸ”— {tweet.url}")
            
            if tweet.media_urls:
                print(f"   ğŸ“· åª’ä½“: {len(tweet.media_urls)} å¼ ")
        
        print("\n" + "=" * 80)
        
    except Exception as e:
        logger.error(f"è¿è¡Œå¤±è´¥: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
